{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions I will be writting for Simple Linear Regression Algorithm. \n",
    "\n",
    "1. Cost Function - I will be using the Logistic Loss to calculate the costs.\n",
    "2. Gradient Function - This function will be used in the gradient descent algorithm to find Gradient. \n",
    "3. Gradient Descent Algorithm - To find the parameters with the least costs.\n",
    "4. Adjusted R square - To know how our algorithm is performing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Importing all the necessary Libraries\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Getting the Data\n",
    "x = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Sigmoid Function \n",
    "def compute_sigmoid(z):\n",
    "    return (1 / (1 + np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Computing Gradient \n",
    "def computing_gradient(x,y,w,b):\n",
    "    m = x.shape[0]\n",
    "    n = x.shape[1]\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(x[i],w) + b\n",
    "        g = compute_sigmoid(z)\n",
    "        error = g - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += (error * x[i,j])\n",
    "        dj_db += error\n",
    "\n",
    "    dj_dw *= (1 / m) \n",
    "    dj_db *= (1 / m)\n",
    "\n",
    "    return dj_dw, dj_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Gradient Descent Algorithm:\n",
    "def gradient_descent(x,y,w,b,alpha,number_of_iters):\n",
    "\n",
    "    for i in range(number_of_iters):\n",
    "        dj_dw, dj_db = computing_gradient(x,y,w,b)\n",
    "        w -= (alpha * dj_dw)\n",
    "        b -= (alpha * dj_db)\n",
    "\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "updated parameters: w:[5.28123029 5.07815608], b:-14.222409982019837\n"
     ]
    }
   ],
   "source": [
    "w  = [0,0]\n",
    "b  = 0.\n",
    "alph = 0.1\n",
    "iters = 10000\n",
    "\n",
    "w_out, b_out = gradient_descent(x, y, w, b, alph, iters) \n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
